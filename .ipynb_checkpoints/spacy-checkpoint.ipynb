{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z-HdwN9zeMF"
   },
   "source": [
    "!pip install yake\n",
    "!pip install matplotlib\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8VnVULf3KX3",
    "outputId": "050a454a-1a0e-48a9-9b09-733a7fde3c84"
   },
   "outputs": [],
   "source": [
    "#pip install yake\n",
    "#pip install matplotlib\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "#pip install spacy textblob autocorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PU19bKWucaDw"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import yake\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itUZlFbQcaD6"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not re.match(r'^[@¦\\x9f\\x92â\\x80]+$', token.text)]\n",
    "    filtered_text = ' '.join(tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpXFdHG5caD_"
   },
   "outputs": [],
   "source": [
    "def tokenizar(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYVhFC_gcaEC"
   },
   "outputs": [],
   "source": [
    "def lemmatizar_tokens(tokens):\n",
    "    doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(doc)]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FCv7ePf0Q0d"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    corrected_text = spell(text)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiRym1At0Stw"
   },
   "outputs": [],
   "source": [
    "def correct_grammar(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkI72gTV0a_P"
   },
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    text = correct_spelling(text)\n",
    "    text = correct_grammar(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4SGWOUF3V2C",
    "outputId": "62b1c279-fa9e-495d-8d6a-c22067456063"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "\n",
    "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
    "\n",
    "# Check TPU availability\n",
    "tpu_available = False\n",
    "devices = tf.config.list_logical_devices()\n",
    "for device in devices:\n",
    "    if device.device_type == 'TPU':\n",
    "        tpu_available = True\n",
    "        break\n",
    "\n",
    "print(\"TPU Available:\", tpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHkbkBcrcaEE"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.head(10)\n",
    "    df.dropna(inplace=True)\n",
    "    df['text_correct'] = df['tweet'].apply(lambda x: correct_text(x))\n",
    "    df['text_processed'] = df['text_correct'].apply(lambda x: remove_stopwords(x))\n",
    "    df['text_tokens'] = df['text_processed'].apply(lambda x: tokenizar(x))\n",
    "    df['text_lemmatized'] = df['text_tokens'].apply(lambda x: lemmatizar_tokens(x))\n",
    "    df['text_final'] = df['text_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "    return df['text_final'], df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bpguo4tWDb4X"
   },
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    plot(y_test,y_pred)\n",
    "    print(\"Acurácia do modelo:\", accuracy)\n",
    "    return clf, vectorizer, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfTlSdaHDIzI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "def plot(y_test,y_pred):\n",
    "  cm = confusion_matrix(y_test,y_pred)\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "                        display_labels = [False, True])\n",
    "  cm_display.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqFdUkZpzN_g"
   },
   "outputs": [],
   "source": [
    "X, y = preprocess_data('Twitter Sentiments.csv')\n",
    "train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OjFE9VucaEF"
   },
   "outputs": [],
   "source": [
    "def plot_keyword_scores(keywords):\n",
    "    keywords.sort(key=lambda x: x[1])\n",
    "    keywords_list, scores_list = zip(*keywords)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(keywords_list, scores_list)\n",
    "    plt.xlabel(\"Keywords\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.title(\"Keyword Scores\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwxvtJDAcaD9"
   },
   "outputs": [],
   "source": [
    "def extract_keywords(X, y, label, language='pt', n=1, k=20):\n",
    "    data_label = X[y == label]\n",
    "    data_label = ' '.join(data_label.astype(str))\n",
    "    keyword_extractor = yake.KeywordExtractor(lan=language, n=n, top=k)\n",
    "    keywords = keyword_extractor.extract_keywords(data_label)\n",
    "    keywords.sort()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQiTro1XzN_i"
   },
   "outputs": [],
   "source": [
    "keywords0 = extract_keywords(X, y, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPN_QpWWcaEG"
   },
   "outputs": [],
   "source": [
    "plot_keyword_scores(keywords0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-WlbdvqVYVT"
   },
   "outputs": [],
   "source": [
    "keywords1 = extract_keywords(X, y, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjSuJcgBWdHG"
   },
   "outputs": [],
   "source": [
    "plot_keyword_scores(keywords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W03u42vOzhaw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (nlp_python)",
   "language": "python",
   "name": "nlp_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
